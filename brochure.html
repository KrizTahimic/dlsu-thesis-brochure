<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Mechanistic Interpretability of Code Correctness in LLMs</title>
  <link rel="stylesheet" href="brochure.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
</head>
<body>
  <!-- PAGE 1: Inside panels (1-2-3) when opened -->
  <div class="page page-inside">

    <!-- Panel 1: Context & Problem -->
    <section class="panel panel-1">
      <div class="panel-header">
        <div class="header-accent"></div>
      </div>
      <div class="panel-content">
        <h2 class="section-title">Context</h2>
        <p>AI-assisted coding has achieved <strong>widespread adoption</strong>:</p>
        <ul class="feature-list">
          <li><span class="stat">30%</span> of AI-suggested code accepted into production<sup>1</sup></li>
          <li>Projected <span class="stat">$1.5T</span> GDP boost by 2030<sup>1</sup></li>
        </ul>

        <h2 class="section-title">The Problem</h2>
        <div class="highlight-box">
          <p>LLMs' <strong>internal mechanisms</strong> for code correctness remain poorly understood.</p>
        </div>
        <ul class="compact-list">
          <li>44% of LLM bugs identical to historical training errors<sup>2</sup></li>
          <li>Only 12.27% accuracy in bug-prone contexts<sup>2</sup></li>
          <li>Critical for <strong>high-stakes systems</strong> (healthcare, banking, military) demanding transparency</li>
        </ul>

        <h2 class="section-title">The Challenge</h2>
        <p class="small-text">Understanding LLMs requires analyzing individual neurons—but neurons are <strong>polysemantic</strong>, responding to multiple unrelated concepts like academic citations, English dialogue, HTTP requests, and Korean text.</p>
        <div class="figure-container full-width">
          <img src="figures/polysemantic neuron.png" alt="Polysemantic Neuron" style="width: 100%;">
        </div>

        <p class="small-text">A hypothesized cause is <strong>superposition</strong>: networks encode more features than available dimensions. The observed model is a low-dimensional projection of a larger, idealized network where features would be disentangled.</p>
        <div class="figure-container full-width">
          <img src="figures/superposition.png" alt="Superposition" style="width: 100%;">
        </div>
      </div>
    </section>

    <!-- Panel 2: Approach -->
    <section class="panel panel-2">
      <div class="panel-header">
        <div class="header-accent"></div>
      </div>
      <div class="panel-content">
        <h2 class="section-title">Our Approach</h2>
        <p><strong>Sparse Autoencoders (SAEs)</strong> address superposition by expanding activations into a higher-dimensional sparse space, decomposing entangled representations into interpretable directions.</p>
        <div class="figure-container">
          <img src="figures/sae.png" alt="Sparse Autoencoder">
          <p class="figure-caption">SAE: Activations → sparse latent space → reconstructed activations</p>
        </div>

        <h2 class="section-title">Methodology Pipeline</h2>
        <p class="small-text">Using 1,000 Python problems from MBPP, we capture residual stream activations at the final prompt token across all layers, then identify two predicting directions (correct/incorrect, via t-statistics) and two steering directions (correct/incorrect, via separation scores).</p>
        <div class="figure-container full-width">
          <img src="figures/methodology.png" alt="PCDGE Methodology" style="width: 100%;">
        </div>
      </div>
    </section>

    <!-- Panel 3: Key Discovery -->
    <section class="panel panel-3">
      <div class="panel-header">
        <div class="header-accent"></div>
      </div>
      <div class="panel-content">
        <h2 class="section-title">Key Discovery</h2>
        <div class="discovery-box">
          <p><strong>Code correctness directions EXIST</strong> in LLM representations, revealing an <strong>asymmetry</strong>.</p>
        </div>

        <h3 class="subsection-title">Identified Directions</h3>
        <table class="feature-table">
          <thead>
            <tr>
              <th>Name</th>
              <th>Direction</th>
              <th>Result</th>
            </tr>
          </thead>
          <tbody>
            <tr class="success-row">
              <td>Incorrect Predicting</td>
              <td>L19-5441</td>
              <td><strong>F1=0.821</strong> &#10003;</td>
            </tr>
            <tr class="fail-row">
              <td>Correct Predicting</td>
              <td>L16-14439</td>
              <td>F1=0.504 &#10007;</td>
            </tr>
            <tr class="success-row">
              <td>Correct Steering</td>
              <td>L16-11225</td>
              <td><strong>4.04%</strong> &gt; 0% ctrl (p&lt;0.001) &#10003;</td>
            </tr>
            <tr class="fail-row">
              <td>Incorrect Steering</td>
              <td>L25-2853</td>
              <td>64.66% &lt; 100% ctrl (p=1.0) &#10007;</td>
            </tr>
          </tbody>
        </table>

        <p class="tradeoff-note">*Note: Correct steering also corrupts 14.66% of initially correct code, suggesting selective application.</p>

        <p class="insight-text"><em>The asymmetry works in our favor: we can detect errors AND steer toward correctness</em></p>

        <div class="logit-lens-mini">
          <div class="logit-lens-header">What does Incorrect-Predicting detect?</div>
          <div class="logit-lens-content">
            <img src="figures/incorrect-predicting.png" alt="Logit Lens Analysis">
            <p class="logit-lens-explanation">The incorrect-predicting direction activates strongly on null indicators and foreign language tokens, patterns the model has learned to associate with semantic errors.</p>
          </div>
        </div>

        <div class="code-example">
          <div class="code-example-header">Correct Steering in Action (L16-11225)</div>
          <p class="code-example-explanation">Despite logit lens showing formatting tokens, steering produces semantic corrections. The <code>dict.fromkeys()</code> one-liner fails to count; steering transforms it into a proper frequency algorithm with explicit iteration.</p>
          <div class="code-example-content">
            <div class="code-block before">
              <span class="code-block-label">Before Steering ✗</span>
              <pre><span class="keyword">def</span> <span class="function">char_frequency</span>(string):
  <span class="keyword">return</span> dict.fromkeys(string, 0)</pre>
            </div>
            <div class="code-block after">
              <span class="code-block-label">After Steering ✓</span>
              <pre><span class="keyword">def</span> <span class="function">char_frequency</span>(string):
  frequency = {}
  <span class="keyword">for</span> char <span class="keyword">in</span> string:
    <span class="keyword">if</span> char <span class="keyword">in</span> frequency:
      frequency[char] += 1
    <span class="keyword">else</span>:
      frequency[char] = 1
  <span class="keyword">return</span> frequency</pre>
            </div>
          </div>
        </div>
      </div>
    </section>
  </div>

  <!-- PAGE 2: Back panels (4-5-6) -->
  <div class="page page-back">

    <!-- Panel 4: Mechanistic Evidence -->
    <section class="panel panel-4">
      <div class="panel-header teal">
        <div class="header-accent"></div>
      </div>
      <div class="panel-content">
        <h2 class="section-title">Mechanistic Analysis</h2>

        <h3 class="subsection-title">Attention Analysis</h3>
        <p class="small-text">Our prompts contain three components:</p>
        <div class="figure-container full-width">
          <img src="figures/template.png" alt="Prompt Template" style="width: 100%;">
        </div>
        <p class="small-text">Where does the model focus attention when steering activates?</p>
        <div class="figure-container full-width">
          <img src="figures/attention_delta_plots.png" alt="Attention Redistribution" style="width: 100%;">
        </div>
        <p class="small-text"><strong>Correct-steering</strong> redirects attention to <strong>test cases</strong> (+15%), while incorrect-steering shifts away (-13%). <em>Implication: Prompting should prioritize test examples.</em></p>

        <h3 class="subsection-title">Weight Orthogonalization</h3>
        <p class="small-text">Is the direction merely correlated, or <strong>necessary</strong>? We surgically remove each from model weights.</p>
        <div class="ortho-grid">
          <div class="ortho-item success">
            <span class="ortho-direction">Correct Steering Removal</span>
            <span class="ortho-label">CORRUPTION RATE</span>
            <span class="ortho-value"><strong>83.6%</strong></span>
            <span class="ortho-note">vs 19% control (4.4&times;) &#10003;</span>
          </div>
          <div class="ortho-item fail">
            <span class="ortho-direction">Incorrect Steering Removal</span>
            <span class="ortho-label">CORRECTION RATE</span>
            <span class="ortho-value"><strong>2.2%</strong></span>
            <span class="ortho-note">&lt; 5.5% control &#10007;</span>
          </div>
        </div>
        <p class="small-text"><em>Correct-steering is <strong>necessary</strong> for generation; incorrect-steering removal doesn't fix errors (asymmetry confirmed).</em></p>

        <h3 class="subsection-title">Persistence Across Fine-tuning</h3>
        <p class="small-text">Do these directions persist from base to instruction-tuned models?</p>
        <div class="persistence-grid">
          <div class="persistence-item">
            <span class="persistence-label">Incorrect-predicting</span>
            <span class="persistence-value">F1: 0.821 &rarr; 0.772</span>
          </div>
          <div class="persistence-item">
            <span class="persistence-label">Correct-steering</span>
            <span class="persistence-value">4.04% &rarr; 2.93% (p&lt;0.001)</span>
          </div>
        </div>
        <p class="small-text"><em>Both directions persist through fine-tuning, confirming these are fundamental mechanisms.</em></p>
      </div>
    </section>

    <!-- Panel 5: Research Contribution & Contact -->
    <section class="panel panel-5">
      <div class="panel-header teal">
        <div class="header-accent"></div>
      </div>
      <div class="panel-content">
        <h2 class="section-title">Research Contribution</h2>
        <ul class="contribution-list">
          <li><strong>First application</strong> of Sparse Autoencoders to mechanistically interpret code correctness in LLMs</li>
          <li><strong>Adapted</strong> mechanistic interpretability techniques from entity recognition<sup>6</sup> to the code generation domain</li>
        </ul>

        <h2 class="section-title">Practical Applications</h2>

        <div class="application-item">
          <h4 class="application-title">1. Error Detection Systems</h4>
          <ul class="application-details">
            <li>Integrate incorrect-predicting directions into development workflows</li>
            <li>Flag AI-generated code before production deployment</li>
            <li>Implementation: IDE plugins, CI/CD pipeline checks, API services</li>
          </ul>
        </div>

        <div class="application-item">
          <h4 class="application-title">2. Prompting Best Practices</h4>
          <ul class="application-details">
            <li>Prioritize test examples over problem descriptions when prompting</li>
            <li>Invest effort in crafting detailed test cases rather than lengthy descriptions</li>
            <li>No model retraining required—simple prompt restructuring</li>
          </ul>
        </div>

        <div class="application-item">
          <h4 class="application-title">3. Selective Steering Pipeline</h4>
          <ul class="application-details">
            <li>Two-stage approach combining prediction and steering</li>
            <li><strong>Stage 1 - Predict:</strong> Use incorrect-predicting direction (L19-5441) to identify likely errors</li>
            <li><strong>Stage 2 - Steer:</strong> Apply correct-steering direction (L16-11225) only on flagged samples</li>
            <li>Avoids corrupting initially correct code through universal steering</li>
          </ul>
        </div>

        <h2 class="section-title">References</h2>
        <ol class="references-list">
          <li>Dohmke et al. (2023). Sea Change in Software Development: Economic and Productivity Analysis of the AI-Powered Developer Lifecycle.</li>
          <li>Guo et al. (2025). LLMs are Bug Replicators: An Empirical Study on LLMs' Capability in Completing Bug-prone Code.</li>
          <li>Bricken et al. (2023). Towards Monosemanticity: Decomposing Language Models With Dictionary Learning.</li>
          <li>Templeton et al. (2024). Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet.</li>
          <li>Lieberum et al. (2024). Gemma Scope: Open Sparse Autoencoders Everywhere All at Once on Gemma 2.</li>
          <li>Ferrando et al. (2024). Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models.</li>
        </ol>

        <div class="contact-section">
          <h3 class="subsection-title">Contact</h3>
          <div class="contact-grid">
            <div class="contact-item">
              <span class="contact-role">Proponent</span>
              <span class="contact-name">Kriz Tahimic</span>
              <span class="contact-email">kriz_tahimic@dlsu.edu.ph</span>
            </div>
            <div class="contact-item">
              <span class="contact-role">Adviser</span>
              <span class="contact-name">Dr. Charibeth K. Cheng</span>
              <span class="contact-email">charibeth.cheng@dlsu.edu.ph</span>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Panel 6: Front Cover -->
    <section class="panel panel-6 cover">
      <div class="cover-background">
        <div class="gradient-overlay"></div>
        <div class="geometric-pattern"></div>
      </div>
      <div class="cover-content">
        <div class="cover-top">
          <h1 class="cover-title">
            <span class="title-line">Mechanistic</span>
            <span class="title-line">Interpretability</span>
            <span class="title-line-small">of Code Correctness in LLMs</span>
          </h1>
          <p class="cover-subtitle">via Sparse Autoencoders</p>
        </div>

        <div class="cover-visual">
          <svg viewBox="0 0 200 150" class="neural-network">
            <!-- Input layer -->
            <circle cx="30" cy="40" r="8" class="node input"/>
            <circle cx="30" cy="75" r="8" class="node input"/>
            <circle cx="30" cy="110" r="8" class="node input"/>

            <!-- Hidden layer -->
            <circle cx="100" cy="30" r="8" class="node hidden"/>
            <circle cx="100" cy="60" r="8" class="node hidden"/>
            <circle cx="100" cy="90" r="8" class="node hidden"/>
            <circle cx="100" cy="120" r="8" class="node hidden"/>

            <!-- Output layer -->
            <circle cx="170" cy="55" r="8" class="node output"/>
            <circle cx="170" cy="95" r="8" class="node output"/>

            <!-- Connections (faded) -->
            <g class="connections">
              <line x1="38" y1="40" x2="92" y2="30"/>
              <line x1="38" y1="40" x2="92" y2="60"/>
              <line x1="38" y1="75" x2="92" y2="60"/>
              <line x1="38" y1="75" x2="92" y2="90"/>
              <line x1="38" y1="110" x2="92" y2="90"/>
              <line x1="38" y1="110" x2="92" y2="120"/>
              <line x1="108" y1="30" x2="162" y2="55"/>
              <line x1="108" y1="60" x2="162" y2="55"/>
              <line x1="108" y1="90" x2="162" y2="95"/>
              <line x1="108" y1="120" x2="162" y2="95"/>
            </g>

            <!-- Highlighted path (correctness direction) -->
            <path d="M 38 75 Q 70 60 92 60 Q 130 60 162 55" class="direction-path" fill="none"/>
            <text x="100" y="145" class="direction-label">correctness direction</text>
          </svg>
        </div>

        <div class="cover-bottom">
          <p class="author-name">Kriz Tahimic</p>
          <p class="adviser-name">Adviser: Dr. Charibeth K. Cheng</p>
          <div class="institution-badge">
            <span class="college">College of Computer Studies</span>
            <span class="university">De La Salle University</span>
          </div>
          <p class="academic-year">Academic Year 2025&ndash;2026</p>
        </div>
      </div>
    </section>
  </div>
</body>
</html>
